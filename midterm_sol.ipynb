{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hacka (midterm) thon \n",
    "\n",
    "## Detecting Malicious URLs \n",
    "\n",
    "Today you are invited to repeat the path of researchers Detecting Malicious URLs.\n",
    "An anonymized 120-day subset of our ICML-09 data set.\n",
    "The data set consists of about 2.4 million URLs (examples) and 3.2 million features. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Download data using link below\n",
    "[Download Dataset](http://www.sysnet.ucsd.edu/projects/url/url_svmlight.tar.gz)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Description of Data (SVM-light)\n",
    "Uncompressing the archive url_svmlight.tar.gz will yield a directory url_svmlight/ containing the following files:\n",
    "\n",
    "1. **FeatureTypes**. A text file list of feature indices that correspond to real-valued features.\n",
    "2. **DayX.svm** (where X is an integer from 0 to 120) --- The data for day X in SVM-light format. A label of +1 corresponds to a malicious URL and -1 corresponds to a benign URL.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Read article\n",
    "Please familiarize yourself with original research article. It will give you required context.\n",
    "\n",
    "*\"**Beyond Blacklists: Learning to Detect Malicious Web Sites from Suspicious URLs**\"* \n",
    "\n",
    "*Justin Ma, Lawrence K. Saul, Stefan Savage, Geoffrey M. Voelker* "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demo part"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Upload data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 123 files\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_svmlight_file\n",
    "files = glob.glob('./url_svmlight/*.svm')\n",
    "print(\"There are %d files\" % len(files))\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. What is inside"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tarfile\n",
    "from sklearn.datasets import load_svmlight_file\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "extracting url_svmlight,f size 0\n",
      "extracting url_svmlight/Day33.svm,f size 18674876\n",
      "extracting url_svmlight/Day32.svm,f size 18599211\n",
      "extracting url_svmlight/Day53.svm,f size 18963938\n",
      "extracting url_svmlight/Day20.svm,f size 18633460\n",
      "extracting url_svmlight/Day7.svm,f size 18777054\n",
      "extracting url_svmlight/Day117.svm,f size 18106370\n",
      "max X = 3231952, max y dimension = 20000\n"
     ]
    }
   ],
   "source": [
    "uri = ('./url_svmlight.tar.gz')\n",
    "tar = tarfile.open(uri, \"r:gz\")\n",
    "max_obs = 0\n",
    "max_vars = 0\n",
    "i = 0\n",
    "split = 5\n",
    "for tarinfo in tar:\n",
    "    print(\"extracting %s,f size %s\" % (tarinfo.name, tarinfo.size))\n",
    "    if tarinfo.isfile():\n",
    "        f = tar.extractfile(tarinfo.name)\n",
    "        X,y = load_svmlight_file(f)\n",
    "        max_vars = np.maximum(max_vars, X.shape[0])\n",
    "        max_obs = np.maximum(max_obs, X.shape[1])\n",
    "    if i > split:\n",
    "        break\n",
    "    i+=1\n",
    "print(\"max X = %s, max y dimension = %s\" % (max_obs, max_vars)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. What is inside"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.99      0.97      0.98     14590\n",
      "           1       0.92      0.98      0.95      5410\n",
      "\n",
      "    accuracy                           0.97     20000\n",
      "   macro avg       0.96      0.97      0.96     20000\n",
      "weighted avg       0.97      0.97      0.97     20000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "classes = [-1,1] # 1_:url- safety, -1: url- non-safety\n",
    "sgd = SGDClassifier(loss='log')\n",
    "n_features = 3231952\n",
    "split = 5\n",
    "i = 0\n",
    "for tarinfo in tar:\n",
    "    if i > split:\n",
    "        break\n",
    "    if tarinfo.isfile():\n",
    "        f = tar.extractfile(tarinfo.name)\n",
    "        X,y = load_svmlight_file(f,n_features=n_features)\n",
    "        if i < split:\n",
    "            sgd.partial_fit(X,y, classes = classes)\n",
    "        if i == split:\n",
    "            print (classification_report(sgd.predict(X),y))\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Midterm (Part 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grading criteria\n",
    "- Complete solution - 60%\n",
    "- F1 Score - 40%\n",
    "    - The first 10 results get 40%\n",
    "    - Worst result get 20%\n",
    "    - All others are on a scale between them"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deadline\n",
    "20:00 MSK, April 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Train, test\n",
    "- Upload data (you can use template above)\n",
    "- Separate your dataset into train and test subsets of observations\n",
    "- Use the 8:2 ratio: 80% train set, 20% test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import random\n",
    "def train_test_split(train_size = 0.8):\n",
    "    \n",
    "    train = open('./url_svmlight/train.svm','w')\n",
    "    test  = open('./url_svmlight/test.svm','w')\n",
    "    summ = 0\n",
    "    for i in range(121):\n",
    "        inn = open('./url_svmlight/Day' + str(i) + '.svm','r')\n",
    "        \n",
    "        print(\"file: \" + str(i))\n",
    "        \n",
    "        q = [0, 0]\n",
    "        \n",
    "        for line in inn:\n",
    "            a = line.split()\n",
    "            if(a[0] == \"-1\"):\n",
    "                q[0] += 1\n",
    "            else:\n",
    "                q[1] += 1\n",
    "        \n",
    "        inn.close()\n",
    "        summ += q[0] + q[1]\n",
    "        \n",
    "        check = [int(train_size * q[0]),int(train_size * q[1])]\n",
    "        start = [0, 0]\n",
    "        \n",
    "        inn = open('./url_svmlight/Day' + str(i) + '.svm','r')\n",
    "        \n",
    "        if (train_size * q[0]) % 1 >=0.5:\n",
    "            check[0] += 1\n",
    "        if (train_size * q[1]) % 1 >=0.5:\n",
    "            check[1] += 1\n",
    "        for line in inn:\n",
    "            a = line.split()\n",
    "\n",
    "            rand = random()\n",
    "            if rand > 0.5:\n",
    "                if a[0] == \"-1\":\n",
    "                    if start[0] + 1 <= check[0]:\n",
    "                        train.write(line)\n",
    "                        train.write(\"\\n\")\n",
    "                        start[0] += 1\n",
    "                    else:\n",
    "                        test.write(line)\n",
    "                        test.write(\"\\n\")\n",
    "                        q[0] -= 1\n",
    "                else:\n",
    "                    if start[1] + 1 <= check[1]:\n",
    "                        train.write(line)\n",
    "                        train.write(\"\\n\")\n",
    "                        start[1] += 1\n",
    "                    else:\n",
    "                        test.write(line)\n",
    "                        test.write(\"\\n\")\n",
    "                        q[1] -= 1\n",
    "            else:\n",
    "                if(a[0] == \"-1\"):\n",
    "                    if q[0] > check[0]:\n",
    "                        test.write(line)\n",
    "                        test.write(\"\\n\")\n",
    "                        q[0] -= 1\n",
    "                    else :\n",
    "                        train.write(line)\n",
    "                        train.write(\"\\n\")\n",
    "                        start[0] += 1\n",
    "                else:\n",
    "                    if q[1] > check[1]:\n",
    "                        test.write(line)\n",
    "                        test.write(\"\\n\")\n",
    "                        q[1] -= 1\n",
    "                    else :\n",
    "                        train.write(line)\n",
    "                        train.write(\"\\n\")\n",
    "                        start[1] += 1\n",
    "        print(\"finish: \" + str(i))\n",
    "    print(summ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file: 0\n",
      "finish: 0\n",
      "file: 1\n",
      "finish: 1\n",
      "file: 2\n",
      "finish: 2\n",
      "file: 3\n",
      "finish: 3\n",
      "file: 4\n",
      "finish: 4\n",
      "file: 5\n",
      "finish: 5\n",
      "file: 6\n",
      "finish: 6\n",
      "file: 7\n",
      "finish: 7\n",
      "file: 8\n",
      "finish: 8\n",
      "file: 9\n",
      "finish: 9\n",
      "file: 10\n",
      "finish: 10\n",
      "file: 11\n",
      "finish: 11\n",
      "file: 12\n",
      "finish: 12\n",
      "file: 13\n",
      "finish: 13\n",
      "file: 14\n",
      "finish: 14\n",
      "file: 15\n",
      "finish: 15\n",
      "file: 16\n",
      "finish: 16\n",
      "file: 17\n",
      "finish: 17\n",
      "file: 18\n",
      "finish: 18\n",
      "file: 19\n",
      "finish: 19\n",
      "file: 20\n",
      "finish: 20\n",
      "file: 21\n",
      "finish: 21\n",
      "file: 22\n",
      "finish: 22\n",
      "file: 23\n",
      "finish: 23\n",
      "file: 24\n",
      "finish: 24\n",
      "file: 25\n",
      "finish: 25\n",
      "file: 26\n",
      "finish: 26\n",
      "file: 27\n",
      "finish: 27\n",
      "file: 28\n",
      "finish: 28\n",
      "file: 29\n",
      "finish: 29\n",
      "file: 30\n",
      "finish: 30\n",
      "file: 31\n",
      "finish: 31\n",
      "file: 32\n",
      "finish: 32\n",
      "file: 33\n",
      "finish: 33\n",
      "file: 34\n",
      "finish: 34\n",
      "file: 35\n",
      "finish: 35\n",
      "file: 36\n",
      "finish: 36\n",
      "file: 37\n",
      "finish: 37\n",
      "file: 38\n",
      "finish: 38\n",
      "file: 39\n",
      "finish: 39\n",
      "file: 40\n",
      "finish: 40\n",
      "file: 41\n",
      "finish: 41\n",
      "file: 42\n",
      "finish: 42\n",
      "file: 43\n",
      "finish: 43\n",
      "file: 44\n",
      "finish: 44\n",
      "file: 45\n",
      "finish: 45\n",
      "file: 46\n",
      "finish: 46\n",
      "file: 47\n",
      "finish: 47\n",
      "file: 48\n",
      "finish: 48\n",
      "file: 49\n",
      "finish: 49\n",
      "file: 50\n",
      "finish: 50\n",
      "file: 51\n",
      "finish: 51\n",
      "file: 52\n",
      "finish: 52\n",
      "file: 53\n",
      "finish: 53\n",
      "file: 54\n",
      "finish: 54\n",
      "file: 55\n",
      "finish: 55\n",
      "file: 56\n",
      "finish: 56\n",
      "file: 57\n",
      "finish: 57\n",
      "file: 58\n",
      "finish: 58\n",
      "file: 59\n",
      "finish: 59\n",
      "file: 60\n",
      "finish: 60\n",
      "file: 61\n",
      "finish: 61\n",
      "file: 62\n",
      "finish: 62\n",
      "file: 63\n",
      "finish: 63\n",
      "file: 64\n",
      "finish: 64\n",
      "file: 65\n",
      "finish: 65\n",
      "file: 66\n",
      "finish: 66\n",
      "file: 67\n",
      "finish: 67\n",
      "file: 68\n",
      "finish: 68\n",
      "file: 69\n",
      "finish: 69\n",
      "file: 70\n",
      "finish: 70\n",
      "file: 71\n",
      "finish: 71\n",
      "file: 72\n",
      "finish: 72\n",
      "file: 73\n",
      "finish: 73\n",
      "file: 74\n",
      "finish: 74\n",
      "file: 75\n",
      "finish: 75\n",
      "file: 76\n",
      "finish: 76\n",
      "file: 77\n",
      "finish: 77\n",
      "file: 78\n",
      "finish: 78\n",
      "file: 79\n",
      "finish: 79\n",
      "file: 80\n",
      "finish: 80\n",
      "file: 81\n",
      "finish: 81\n",
      "file: 82\n",
      "finish: 82\n",
      "file: 83\n",
      "finish: 83\n",
      "file: 84\n",
      "finish: 84\n",
      "file: 85\n",
      "finish: 85\n",
      "file: 86\n",
      "finish: 86\n",
      "file: 87\n",
      "finish: 87\n",
      "file: 88\n",
      "finish: 88\n",
      "file: 89\n",
      "finish: 89\n",
      "file: 90\n",
      "finish: 90\n",
      "file: 91\n",
      "finish: 91\n",
      "file: 92\n",
      "finish: 92\n",
      "file: 93\n",
      "finish: 93\n",
      "file: 94\n",
      "finish: 94\n",
      "file: 95\n",
      "finish: 95\n",
      "file: 96\n",
      "finish: 96\n",
      "file: 97\n",
      "finish: 97\n",
      "file: 98\n",
      "finish: 98\n",
      "file: 99\n",
      "finish: 99\n",
      "file: 100\n",
      "finish: 100\n",
      "file: 101\n",
      "finish: 101\n",
      "file: 102\n",
      "finish: 102\n",
      "file: 103\n",
      "finish: 103\n",
      "file: 104\n",
      "finish: 104\n",
      "file: 105\n",
      "finish: 105\n",
      "file: 106\n",
      "finish: 106\n",
      "file: 107\n",
      "finish: 107\n",
      "file: 108\n",
      "finish: 108\n",
      "file: 109\n",
      "finish: 109\n",
      "file: 110\n",
      "finish: 110\n",
      "file: 111\n",
      "finish: 111\n",
      "file: 112\n",
      "finish: 112\n",
      "file: 113\n",
      "finish: 113\n",
      "file: 114\n",
      "finish: 114\n",
      "file: 115\n",
      "finish: 115\n",
      "file: 116\n",
      "finish: 116\n",
      "file: 117\n",
      "finish: 117\n",
      "file: 118\n",
      "finish: 118\n",
      "file: 119\n",
      "finish: 119\n",
      "file: 120\n",
      "finish: 120\n",
      "2396130\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_svmlight_file\n",
    "from sklearn.linear_model import Perceptron\n",
    "import numpy as np\n",
    "\n",
    "train_test_split()\n",
    "\n",
    "data = None\n",
    "n_features = 3231961\n",
    "\n",
    "\n",
    "data, target = load_svmlight_file(\"./url_svmlight/train.svm\",n_features=n_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 1)\t1.0\n",
      "  (0, 3)\t0.0912863\n",
      "  (0, 4)\t0.144828\n",
      "  (0, 5)\t0.117647\n",
      "  (0, 9)\t1.0\n",
      "  (0, 10)\t0.142857\n",
      "  (0, 16)\t0.760482\n",
      "  (0, 17)\t0.820882\n",
      "  (0, 18)\t0.150678\n",
      "  (0, 20)\t0.142856\n",
      "  (0, 21)\t0.142857\n",
      "  (0, 23)\t1.0\n",
      "  (0, 27)\t1.0\n",
      "  (0, 32)\t0.111111\n",
      "  (0, 43)\t1.0\n",
      "  (0, 53)\t1.0\n",
      "  (0, 55)\t1.0\n",
      "  (0, 61)\t1.0\n",
      "  (0, 63)\t1.0\n",
      "  (0, 65)\t1.0\n",
      "  (0, 67)\t1.0\n",
      "  (0, 69)\t1.0\n",
      "  (0, 71)\t1.0\n",
      "  (0, 73)\t1.0\n",
      "  (0, 75)\t1.0\n",
      "  :\t:\n",
      "  (1916903, 155178)\t1.0\n",
      "  (1916903, 155179)\t1.0\n",
      "  (1916903, 155180)\t1.0\n",
      "  (1916903, 155181)\t1.0\n",
      "  (1916903, 155182)\t1.0\n",
      "  (1916903, 155193)\t1.0\n",
      "  (1916903, 155194)\t1.0\n",
      "  (1916903, 155195)\t1.0\n",
      "  (1916903, 155196)\t1.0\n",
      "  (1916903, 155197)\t1.0\n",
      "  (1916903, 155198)\t1.0\n",
      "  (1916903, 155199)\t1.0\n",
      "  (1916903, 155200)\t1.0\n",
      "  (1916903, 155201)\t1.0\n",
      "  (1916903, 155202)\t1.0\n",
      "  (1916903, 155203)\t1.0\n",
      "  (1916903, 155204)\t1.0\n",
      "  (1916903, 155205)\t1.0\n",
      "  (1916903, 155206)\t1.0\n",
      "  (1916903, 155207)\t1.0\n",
      "  (1916903, 155208)\t1.0\n",
      "  (1916903, 155209)\t1.0\n",
      "  (1916903, 155210)\t1.0\n",
      "  (1916903, 155211)\t1.0\n",
      "  (1916903, 155212)\t1.0\n"
     ]
    }
   ],
   "source": [
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1. -1. -1. ...  1. -1. -1.]\n"
     ]
    }
   ],
   "source": [
    "print(target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Find out whether it is possible to reduce the dimension?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from scipy.sparse import csr_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data = StandardScaler(with_mean=False).fit_transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    " data_sparse = csr_matrix(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 1)\t1.0\n",
      "  (0, 3)\t0.0912863\n",
      "  (0, 4)\t0.144828\n",
      "  (0, 5)\t0.117647\n",
      "  (0, 9)\t1.0\n",
      "  (0, 10)\t0.142857\n",
      "  (0, 16)\t0.760482\n",
      "  (0, 17)\t0.820882\n",
      "  (0, 18)\t0.150678\n",
      "  (0, 20)\t0.142856\n",
      "  (0, 21)\t0.142857\n",
      "  (0, 23)\t1.0\n",
      "  (0, 27)\t1.0\n",
      "  (0, 32)\t0.111111\n",
      "  (0, 43)\t1.0\n",
      "  (0, 53)\t1.0\n",
      "  (0, 55)\t1.0\n",
      "  (0, 61)\t1.0\n",
      "  (0, 63)\t1.0\n",
      "  (0, 65)\t1.0\n",
      "  (0, 67)\t1.0\n",
      "  (0, 69)\t1.0\n",
      "  (0, 71)\t1.0\n",
      "  (0, 73)\t1.0\n",
      "  (0, 75)\t1.0\n",
      "  :\t:\n",
      "  (1916903, 155178)\t1.0\n",
      "  (1916903, 155179)\t1.0\n",
      "  (1916903, 155180)\t1.0\n",
      "  (1916903, 155181)\t1.0\n",
      "  (1916903, 155182)\t1.0\n",
      "  (1916903, 155193)\t1.0\n",
      "  (1916903, 155194)\t1.0\n",
      "  (1916903, 155195)\t1.0\n",
      "  (1916903, 155196)\t1.0\n",
      "  (1916903, 155197)\t1.0\n",
      "  (1916903, 155198)\t1.0\n",
      "  (1916903, 155199)\t1.0\n",
      "  (1916903, 155200)\t1.0\n",
      "  (1916903, 155201)\t1.0\n",
      "  (1916903, 155202)\t1.0\n",
      "  (1916903, 155203)\t1.0\n",
      "  (1916903, 155204)\t1.0\n",
      "  (1916903, 155205)\t1.0\n",
      "  (1916903, 155206)\t1.0\n",
      "  (1916903, 155207)\t1.0\n",
      "  (1916903, 155208)\t1.0\n",
      "  (1916903, 155209)\t1.0\n",
      "  (1916903, 155210)\t1.0\n",
      "  (1916903, 155211)\t1.0\n",
      "  (1916903, 155212)\t1.0\n"
     ]
    }
   ],
   "source": [
    " print(data_sparse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsvd = TruncatedSVD(n_components=80)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_sparse_tsvd = tsvd.fit(data_sparse).transform(data_sparse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_tsvd = tsvd.fit(data).transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original number of sparsed features: 3231961\n",
      "Original number of features: 3231961\n",
      "Reduced number of sparsed features: 80\n",
      "Reduced number of features: 80\n"
     ]
    }
   ],
   "source": [
    "print(\"Original number of sparsed features:\", data_sparse.shape[1])\n",
    "print(\"Original number of features:\", data.shape[1])\n",
    "print(\"Reduced number of sparsed features:\", data_sparse_tsvd.shape[1])\n",
    "print(\"Reduced number of features:\", data_tsvd.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Create a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PassiveAggressiveClassifier()"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import PassiveAggressiveClassifier\n",
    "pac = PassiveAggressiveClassifier()\n",
    "pac.fit(data,target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Get the quality\n",
    "- precision\n",
    "- recall\n",
    "- f1-score\n",
    "- support "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "        -1.0   0.991957  0.995866  0.993908    319536\n",
      "         1.0   0.991662  0.983844  0.987737    159690\n",
      "\n",
      "    accuracy                       0.991860    479226\n",
      "   macro avg   0.991810  0.989855  0.990823    479226\n",
      "weighted avg   0.991859  0.991860  0.991852    479226\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "test_data,test_target = load_svmlight_file(\"./url_svmlight/test.svm\",n_features=n_features)\n",
    "#the accuracy before dimension reduction \n",
    "print (classification_report(pac.predict(test_data),test_target, digits = 6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PassiveAggressiveClassifier()"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Dimension Reduction without using sparse matrix\n",
    "pac.fit(data_tsvd,target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_tsvd = tsvd.transform(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "        -1.0   0.961870  0.980483  0.971087    314705\n",
      "         1.0   0.961232  0.925651  0.943106    164521\n",
      "\n",
      "    accuracy                       0.961659    479226\n",
      "   macro avg   0.961551  0.953067  0.957097    479226\n",
      "weighted avg   0.961651  0.961659  0.961481    479226\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print (classification_report(pac.predict(test_data_tsvd),test_target, digits = 6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PassiveAggressiveClassifier()"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Dimension Reduction with using sparse matrix\n",
    "pac.fit(data_sparse_tsvd,target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_data = StandardScaler(with_mean=False).fit_transform(test_data)\n",
    "test_data_sparse = csr_matrix(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_sparse_tsvd = tsvd.transform(test_data_sparse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####the accuracy after dimension reduction using sparse matrix#######################\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "        -1.0   0.983307  0.947908  0.965283    332775\n",
      "         1.0   0.890583  0.963435  0.925578    146451\n",
      "\n",
      "    accuracy                       0.952653    479226\n",
      "   macro avg   0.936945  0.955671  0.945430    479226\n",
      "weighted avg   0.954971  0.952653  0.953149    479226\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#1-PassiveAggressiveClassifier\n",
    "print (classification_report(pac.predict(test_data_sparse_tsvd),test_target, digits = 6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the accuracy is not good with using sparse matrix before using dimension reduction \n",
    "#such that accuracy with sparse matrix 95% but without using sparse matrix 96% in case of PassiveAggressiveClassifier\n",
    "#So,I will use dimension reduction without sparse matrix with different classification models to compare the accuracy with\n",
    "#different classifiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2-Decision tree\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "model_tree=DecisionTreeClassifier(max_depth=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "        -1.0   0.993161  0.992186  0.992673    321110\n",
      "         1.0   0.984163  0.986124  0.985143    158116\n",
      "\n",
      "    accuracy                       0.990186    479226\n",
      "   macro avg   0.988662  0.989155  0.988908    479226\n",
      "weighted avg   0.990192  0.990186  0.990189    479226\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#without using dimension reduction\n",
    "model_tree.fit(data,target)\n",
    "print (classification_report(model_tree.predict(test_data),test_target, digits = 6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "        -1.0   0.986839  0.984274  0.985555    321631\n",
      "         1.0   0.968074  0.973210  0.970635    157595\n",
      "\n",
      "    accuracy                       0.980635    479226\n",
      "   macro avg   0.977457  0.978742  0.978095    479226\n",
      "weighted avg   0.980668  0.980635  0.980648    479226\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#with using dimension reduction\n",
    "model_tree.fit(data_tsvd, target)\n",
    "print (classification_report(model_tree.predict(test_data_tsvd),test_target, digits = 6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#3-Logistic regression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "model_lr = LogisticRegression(max_iter=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "        -1.0   0.985686  0.987628  0.986656    320164\n",
      "         1.0   0.974999  0.971131  0.973061    159062\n",
      "\n",
      "    accuracy                       0.982152    479226\n",
      "   macro avg   0.980342  0.979379  0.979858    479226\n",
      "weighted avg   0.982138  0.982152  0.982144    479226\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#without using dimension reduction\n",
    "model_lr.fit(data, target)\n",
    "print (classification_report(model_lr.predict(test_data),test_target, digits = 6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "        -1.0   0.972366  0.977546  0.974949    319095\n",
      "         1.0   0.954775  0.944639  0.949680    160131\n",
      "\n",
      "    accuracy                       0.966550    479226\n",
      "   macro avg   0.963570  0.961092  0.962314    479226\n",
      "weighted avg   0.966488  0.966550  0.966505    479226\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#with using dimension reduction\n",
    "model_lr.fit(data_tsvd, target)\n",
    "print (classification_report(model_lr.predict(test_data_tsvd),test_target, digits = 6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#4-Random Forest\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "model_rf = RandomForestClassifier(max_depth=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "        -1.0   0.999857  0.752039  0.858420    426506\n",
      "         1.0   0.332473  0.999127  0.498923     52720\n",
      "\n",
      "    accuracy                       0.779221    479226\n",
      "   macro avg   0.666165  0.875583  0.678671    479226\n",
      "weighted avg   0.926437  0.779221  0.818871    479226\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#without using dimension reduction\n",
    "model_rf.fit(data, target)\n",
    "print (classification_report(model_rf.predict(test_data),test_target, digits = 6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "        -1.0   0.990365  0.988359  0.989361    321446\n",
      "         1.0   0.976381  0.980409  0.978391    157780\n",
      "\n",
      "    accuracy                       0.985742    479226\n",
      "   macro avg   0.983373  0.984384  0.983876    479226\n",
      "weighted avg   0.985761  0.985742  0.985749    479226\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#with using dimension reduction\n",
    "model_rf.fit(data_tsvd, target)\n",
    "print (classification_report(model_rf.predict(test_data_tsvd),test_target, digits = 6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#5-Support Vector Classification\n",
    "from sklearn.svm import SVC\n",
    "model_svc= SVC(max_iter=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "        -1.0   0.907505  0.876169  0.891562    332268\n",
      "         1.0   0.740297  0.798092  0.768109    146958\n",
      "\n",
      "    accuracy                       0.852226    479226\n",
      "   macro avg   0.823901  0.837131  0.829835    479226\n",
      "weighted avg   0.856229  0.852226  0.853704    479226\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#without using dimension reduction\n",
    "model_svc.fit(data, target)\n",
    "print (classification_report( model_svc.predict(test_data),test_target, digits = 6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "        -1.0   0.559862  0.767031  0.647274    234151\n",
      "         1.0   0.655686  0.423874  0.514892    245075\n",
      "\n",
      "    accuracy                       0.591541    479226\n",
      "   macro avg   0.607774  0.595453  0.581083    479226\n",
      "weighted avg   0.608866  0.591541  0.579574    479226\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#with using dimension reduction\n",
    "model_svc.fit(data_tsvd, target)\n",
    "print (classification_report( model_svc.predict(test_data_tsvd),test_target, digits = 6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Report\n",
    "             #1-PassiveAggressiveClassifier 2-Decisiontree    3-Logisticregression  4-Randomforest    5-SVC\n",
    "#without DR      99%                          99%                 98%                    77.9%            85%\n",
    "#with DR         96%                          98%                 96.7%                  98.6%            59%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the best accuracy by using Decision tree classifier with using dimension reduction is 98%\n",
    "#but with out using dimension reduction is 99%.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nthe dimension reduction achieved good results in case of Random Forest from 77.8% to 98.6% but with other classifiers models\\nthe accuracy is reduced about 1% by using decision tree which is achieved after using dimension reduction.\\nSo, I think we don't need to do dimension reduction in this case from dataset with most of classifiers but in case of using\\nrandom forest classifier we need to do dimension reduction.\\n\""
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "the dimension reduction achieved good results in case of Random Forest from 77.8% to 98.6% but with other classifiers models\n",
    "the accuracy is reduced about 1% by using decision tree which is achieved after using dimension reduction.\n",
    "So, I think we don't need to do dimension reduction in this case from dataset with most of classifiers but in case of using\n",
    "random forest classifier we need to do dimension reduction.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
